{
  "filename": "system-adaptive-protection.md",
  "__html": "<h1>系统自适应保护</h1>\n<p>Sentinel 系统自适应保护从整体维度对应用入口流量进行控制，结合应用的 Load、总体平均 RT、入口 QPS 和线程数等几个维度的监控指标，让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。</p>\n<h2>背景</h2>\n<p>在开始之前，先回顾一下 Sentinel 做系统自适应保护的目的：</p>\n<ul>\n<li>保证系统不被拖垮</li>\n<li>在系统稳定的前提下，保持系统的吞吐量</li>\n</ul>\n<p>长期以来，系统自适应保护的思路是根据硬指标，即系统的负载 (load1) 来做系统过载保护。当系统负载高于某个阈值，就禁止或者减少流量的进入；当 load 开始好转，则恢复流量的进入。这个思路给我们带来了不可避免的两个问题：</p>\n<ul>\n<li>load 是一个“果”，如果根据 load 的情况来调节流量的通过率，那么就始终有延迟性。也就意味着通过率的任何调整，都会过一段时间才能看到效果。当前通过率是使 load 恶化的一个动作，那么也至少要过 1 秒之后才能观测到；同理，如果当前通过率调整是让 load 好转的一个动作，也需要 1 秒之后才能继续调整，这样就浪费了系统的处理能力。所以我们看到的曲线，总是会有抖动。</li>\n<li>恢复慢。想象一下这样的一个场景（真实），出现了这样一个问题，下游应用不可靠，导致应用 RT 很高，从而 load 到了一个很高的点。过了一段时间之后下游应用恢复了，应用 RT 也相应减少。这个时候，其实应该大幅度增大流量的通过率；但是由于这个时候 load 仍然很高，通过率的恢复仍然不高。</li>\n</ul>\n<p><a href=\"https://en.wikipedia.org/wiki/TCP_congestion_control#TCP_BBR\">TCP BBR</a> 的思想给了我们一个很大的启发。我们应该根据系统能够处理的请求，和允许进来的请求，来做平衡，而不是根据一个间接的指标（系统 load）来做限流。最终我们追求的目标是 <strong>在系统不被拖垮的情况下，提高系统的吞吐率，而不是 load 一定要到低于某个阈值</strong>。如果我们还是按照固有的思维，超过特定的 load 就禁止流量进入，系统 load 恢复就放开流量，这样做的结果是无论我们怎么调参数，调比例，都是按照果来调节因，都无法取得良好的效果。</p>\n<p>Sentinel 在系统自适应保护的做法是，用 load1 作为启动控制流量的值，而允许通过的流量由处理请求的能力，即请求的响应时间以及当前系统正在处理的请求速率来决定。</p>\n<h2>系统规则</h2>\n<p>系统保护规则是从应用级别的入口流量进行控制，从单台机器的总体 Load、RT、入口 QPS 和线程数四个维度监控应用数据，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。</p>\n<p>系统保护规则是应用整体维度的，而不是资源维度的，并且<strong>仅对入口流量生效</strong>。入口流量指的是进入应用的流量（<code>EntryType.IN</code>），比如 Web 服务或 Dubbo 服务端接收的请求，都属于入口流量。</p>\n<p>系统规则支持以下的阈值类型：</p>\n<ul>\n<li><strong>Load</strong>（仅对 Linux/Unix-like 机器生效）：当系统 load1 超过阈值，且系统当前的并发线程数超过系统容量时才会触发系统保护。系统容量由系统的 <code>maxQps * minRt</code> 计算得出。设定参考值一般是 <code>CPU cores * 2.5</code>。</li>\n<li><strong>CPU usage</strong>（1.5.0+ 版本）：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0）。</li>\n<li><strong>RT</strong>：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。</li>\n<li><strong>线程数</strong>：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。</li>\n<li><strong>入口 QPS</strong>：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。</li>\n</ul>\n<h2>原理</h2>\n<p>先用经典图来镇楼:</p>\n<p><img src=\"https://user-images.githubusercontent.com/9434884/50813887-bff10300-1352-11e9-9201-437afea60a5a.png\" alt=\"TCP-BBR-pipe\"></p>\n<p>我们把系统处理请求的过程想象为一个水管，到来的请求是往这个水管灌水，当系统处理顺畅的时候，请求不需要排队，直接从水管中穿过，这个请求的RT是最短的；反之，当请求堆积的时候，那么处理请求的时间则会变为：排队时间 + 最短处理时间。</p>\n<ul>\n<li>推论一:  如果我们能够保证水管里的水量，能够让水顺畅的流动，则不会增加排队的请求；也就是说，这个时候的系统负载不会进一步恶化。</li>\n</ul>\n<p>我们用 T 来表示(水管内部的水量)，用RT来表示请求的处理时间，用P来表示进来的请求数，那么一个请求从进入水管道到从水管出来，这个水管会存在 <code>P * RT</code>　个请求。换一句话来说，当 <code>T ≈ QPS * Avg(RT)</code> 的时候，我们可以认为系统的处理能力和允许进入的请求个数达到了平衡，系统的负载不会进一步恶化。</p>\n<p>接下来的问题是，水管的水位是可以达到了一个平衡点，但是这个平衡点只能保证水管的水位不再继续增高，但是还面临一个问题，就是在达到平衡点之前，这个水管里已经堆积了多少水。如果之前水管的水已经在一个量级了，那么这个时候系统允许通过的水量可能只能缓慢通过，RT会大，之前堆积在水管里的水会滞留；反之，如果之前的水管水位偏低，那么又会浪费了系统的处理能力。</p>\n<ul>\n<li>推论二:　当保持入口的流量是水管出来的流量的最大的值的时候，可以最大利用水管的处理能力。</li>\n</ul>\n<p>然而，和 TCP BBR 的不一样的地方在于，还需要用一个系统负载的值（load1）来激发这套机制启动。</p>\n<blockquote>\n<p>注：这种系统自适应算法对于低 load 的请求，它的效果是一个“兜底”的角色。<strong>对于不是应用本身造成的 load 高的情况（如其它进程导致的不稳定的情况），效果不明显。</strong></p>\n</blockquote>\n<h2>示例</h2>\n<p>我们提供了系统自适应限流的示例：<a href=\"https://github.com/alibaba/Sentinel/blob/master/sentinel-demo/sentinel-demo-basic/src/main/java/com/alibaba/csp/sentinel/demo/system/SystemGuardDemo.java\">SystemGuardDemo</a>。</p>\n"
}